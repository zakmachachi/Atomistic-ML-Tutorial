{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643a74cb",
   "metadata": {},
   "source": [
    "## Load the conda environment from our last class\n",
    "\n",
    "```bash\n",
    "conda activate atomistic-ml-class\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73cdd7",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "## Introduction to machine learning interatomic potentials\n",
    "\n",
    "In this part, we will learn how to train and use machine learning interatomic potentials (MLIPs) to run molecular dynamics. Specifically we will use graph neural networks (GNNs) as our frameworks for our MLIPs, focusing on two lightweight yet expressive models: PaiNN and SchNet architectures. You can find the original papers for these architectures here:\n",
    "\n",
    "- [PaiNN](https://arxiv.org/abs/2102.03150)\n",
    "- [SchNet](https://pubs.aip.org/aip/jcp/article/148/24/241722/962591/SchNet-A-deep-learning-architecture-for-molecules)\n",
    "\n",
    "Unlike descriptor-based models, GNNs learn directly from atomic graphs with position and feature information.\n",
    "\n",
    "Both can be trained on consumer hardware (e.g., laptops), making them suitable for hands-on learning.\n",
    "\n",
    "We will be using graph-pes to handle training and running the models. graph-pes is a Python package that provides a simple interface for training and using GNNs for interatomic potentials. You can find the documentation for graph-pes here:\n",
    "\n",
    "- [graph-pes documentation](https://jla-gardner.github.io/graph-pes/)\n",
    "\n",
    "Take a look at the training documentation here:\n",
    "\n",
    "- [graph-pes training documentation](https://jla-gardner.github.io/graph-pes/cli/graph-pes-train/complete-docs.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_atoms import load_dataset\n",
    "\n",
    "# load structures and split the data into training, validation and test\n",
    "structures = load_dataset(\"../Class-1/structures_filt.xyz\")\n",
    "\n",
    "# alternatively, you can load the C-GAP-17 dataset\n",
    "# structures = load_dataset(\"C-GAP-17\")\n",
    "\n",
    "\n",
    "train, val, test = structures.random_split([0.8, 0.1, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4178f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import write\n",
    "\n",
    "write(\"train.xyz\", train)\n",
    "write(\"valid.xyz\", val)\n",
    "write(\"test.xyz\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29570d27",
   "metadata": {},
   "source": [
    "We provide two configuration input files (in `.yaml` format) for `graphPES` for SchNet and NequIP. Check out their structure, and finish the missing parts to train your first GNN MLIP! Feel free to experiment with different values for hyperparameters such as the `cutoff`, the number of `radial_features` and the number of `layers`. Change the number of training configurations between 10 and 500 to construct the learning curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dce38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[graph-pes INFO]: Started `graph-pes-train` at 2025-05-23 12:05:21.580\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chihebbenmahmoud/source/miniconda3/envs/graphPES/lib/python3.10/site-packages/graph_pes/config/shared.py\", line 78, in instantiate_config_from_dict\n",
      "    dacite.from_dict(\n",
      "  File \"/Users/chihebbenmahmoud/source/miniconda3/envs/graphPES/lib/python3.10/site-packages/dacite/core.py\", line 69, in from_dict\n",
      "    value = _build_value(type_=field_type, data=data[key], config=config)\n",
      "  File \"/Users/chihebbenmahmoud/source/miniconda3/envs/graphPES/lib/python3.10/site-packages/dacite/core.py\", line 107, in _build_value\n",
      "    data = from_dict(data_class=type_, data=data, config=config)\n",
      "  File \"/Users/chihebbenmahmoud/source/miniconda3/envs/graphPES/lib/python3.10/site-packages/dacite/core.py\", line 74, in from_dict\n",
      "    raise WrongTypeError(field_path=field.name, field_type=field_type, value=value)\n",
      "dacite.exceptions.WrongTypeError: wrong value type for field \"data.train\" - should be \"GraphDataset\" instead of value \"{'path': 'train.xyz', 'n': '436 number of structures to use', 'shuffle': False}\" of type \"dict\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chihebbenmahmoud/source/miniconda3/envs/graphPES/bin/graph-pes-train\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/chihebbenmahmoud/source/miniconda3/envs/graphPES/lib/python3.10/site-packages/graph_pes/scripts/train.py\", line 312, in main\n",
      "    train_from_config(config_dict)\n",
      "  File \"/Users/chihebbenmahmoud/source/miniconda3/envs/graphPES/lib/python3.10/site-packages/graph_pes/scripts/train.py\", line 82, in train_from_config\n",
      "    config_data, config = instantiate_config_from_dict(\n",
      "  File \"/Users/chihebbenmahmoud/source/miniconda3/envs/graphPES/lib/python3.10/site-packages/graph_pes/config/shared.py\", line 85, in instantiate_config_from_dict\n",
      "    raise ValueError(\n",
      "ValueError: Failed to instantiate a config object of type <class 'graph_pes.config.training.TrainingConfig'> from the following dictionary:\n",
      "general: \n",
      "  seed: 42\n",
      "  root_dir: graph-pes-results\n",
      "  run_id: train-PaiNN\n",
      "  log_level: INFO\n",
      "  progress: logged\n",
      "  torch: \n",
      "    dtype: float32\n",
      "    float32_matmul_precision: high\n",
      "fitting: \n",
      "  pre_fit_model: True\n",
      "  max_n_pre_fit: 5000\n",
      "  trainer_kwargs: \n",
      "    max_epochs: 250\n",
      "    accelerator: auto\n",
      "    enable_model_summary: False\n",
      "    check_val_every_n_epoch: 1\n",
      "  loader_kwargs: \n",
      "    num_workers: 1\n",
      "    persistent_workers: True\n",
      "    batch_size: 16\n",
      "    pin_memory: False\n",
      "  callbacks: []\n",
      "  scheduler: \n",
      "    name: ReduceLROnPlateau\n",
      "    factor: 0.5\n",
      "    patience: 10\n",
      "  swa: None\n",
      "  early_stopping: None\n",
      "  early_stopping_patience: None\n",
      "  optimizer: \n",
      "    name: AdamW\n",
      "    lr: 0.01\n",
      "wandb: None\n",
      "CUTOFF: 3.7\n",
      "PATH_TRAIN: train.xyz\n",
      "PATH_TEST: test.xyz\n",
      "PATH_VALID: valid.xyz\n",
      "model: \n",
      "  offset: \n",
      "    +FixedOffset: \n",
      "      C: -148.314002\n",
      "  many-body: \n",
      "    +PaiNN: \n",
      "      cutoff: 3.7\n",
      "      channels: 16\n",
      "      radial_features: 15\n",
      "      layers: 3\n",
      "data: \n",
      "  train: \n",
      "    path: train.xyz\n",
      "    n: 436 number of structures to use\n",
      "    shuffle: False\n",
      "  valid: valid.xyz\n",
      "  test: test.xyz\n",
      "loss: ['+PerAtomEnergyLoss()', '+ForceRMSE()']\n"
     ]
    }
   ],
   "source": [
    "# train a grapPES model from the command line\n",
    "# if training with the C-GAP-17 dataset, set the number of training configuration to ~1000\n",
    "\n",
    "!graph-pes-train train_SchNet.yaml general/run_id=train-SchNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7c6a6",
   "metadata": {},
   "source": [
    "Once the model(s) has finished training, you can load it and evaluate its performance on the training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1342066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graphPES models\n",
    "from graph_pes.models import load_model\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model = (\n",
    "    load_model(\"/path/to/trained/mode\")  # load the model\n",
    "    .to(device)  # move to GPU if available\n",
    "    .eval()  # set to evaluation mode\n",
    ")\n",
    "\n",
    "# setup ASE calculator\n",
    "calculator = best_model.ase_calculator()\n",
    "\n",
    "# example of getting energies and forces using the calculator\n",
    "frm = test[0]\n",
    "frm.calc = calculator\n",
    "energy_pred = frm.get_potential_energy()\n",
    "froces_pred = frm.get_forces()\n",
    "\n",
    "# perform the same for all structures in the test set and find the performance meteric like you did in Day 1\n",
    "# examine scatter plots of reference energies and forces vs ML predicted quantities\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c874d",
   "metadata": {},
   "source": [
    "Now you can redo the same steps to train a NequIP model (takes ~3 times longer on CPU to obtain a more accurate MLIP). To do so, use the configuration file `train_NequIP.yaml`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d6c58",
   "metadata": {},
   "source": [
    "# Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787b2c3",
   "metadata": {},
   "source": [
    "In this sectino, we will be running molecular dynamics (MD) simulations using the trained MLIPs of Part 1 and the Atomic Simulation Environment (ASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88f2da",
   "metadata": {},
   "source": [
    "First of all, we test our the stability of the trained MLIPs by annealing (holding at constant temperature) a few starting configurations at 300K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from Day 1:\n",
    "\n",
    "starting_configs = load_atoms(\"/path/to/first/day/carbon/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained MLIP\n",
    "\n",
    "from graph_pes.models import load_model\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model = (\n",
    "    load_model(\"/path/to/trained/mode\")  # load the model\n",
    "    .to(device)  # move to GPU if available\n",
    "    .eval()  # set to evaluation mode\n",
    ")\n",
    "\n",
    "# setup ASE calculator\n",
    "calculator = best_model.ase_calculator()\n",
    "\n",
    "# test with a few indices for differenet densities and starting configurations\n",
    "initial_frame = starting_configs[0]\n",
    "\n",
    "# setup calculator\n",
    "initial_frame.calc = calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3768e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run MD\n",
    "\n",
    "import ase\n",
    "from ase.md.velocitydistribution import (\n",
    "    MaxwellBoltzmannDistribution,\n",
    "    ZeroRotation,\n",
    "    Stationary,\n",
    ")\n",
    "from ase.md import MDLogger\n",
    "from ase.md.nvtberendsen import NVTBerendsen\n",
    "\n",
    "Tinit = 300  # K\n",
    "\n",
    "md_params = {\n",
    "    \"timestep\": 0.5 * ase.units.fs,  # MD timestep\n",
    "    \"taut\": 100 * 0.5 * ase.units.fs,  # thermostat time constant\n",
    "}\n",
    "total_md_steps = ...  # make sure change this\n",
    "\n",
    "# initialize velocities\n",
    "MaxwellBoltzmannDistribution(initial_frame, temperature_K=Tinit)\n",
    "Stationary(initial_frame)\n",
    "ZeroRotation(initial_frame)\n",
    "\n",
    "# initialize dynamics object\n",
    "dyn = NVTBerendsen(initial_frame, temperature_K=Tinit, **md_params)\n",
    "\n",
    "\n",
    "# write trajectory function\n",
    "def write_frame():\n",
    "    dyn.atoms.write(\n",
    "        f\"/path/to/produced/trajectory.xyz\", append=True\n",
    "    )  # make sure the extension is xyz\n",
    "\n",
    "\n",
    "dyn.attach(write_frame, interval=100)  # set the frequency of writing to trajctory file\n",
    "\n",
    "# setup the logger\n",
    "dyn.attach(\n",
    "    MDLogger(\n",
    "        dyn,  # dynamics object\n",
    "        initial_frame,  # intial configuration\n",
    "        f\"path/to/log\",\n",
    "        peratom=True,\n",
    "        mode=\"a\",\n",
    "    ),\n",
    "    interval=100,  # frequency of writing the log\n",
    ")\n",
    "\n",
    "# run the MD\n",
    "dyn.run(total_md_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282ea65",
   "metadata": {},
   "source": [
    "Taking the last frame from the previous simulation, perform a metl-quench simulation. Below we provide the code to perform such simulation. Perform the simulation and answer questions 3-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519245a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a melt-quench\n",
    "# let's run MD\n",
    "\n",
    "import ase\n",
    "from ase.md.velocitydistribution import (\n",
    "    MaxwellBoltzmannDistribution,\n",
    "    ZeroRotation,\n",
    "    Stationary,\n",
    ")\n",
    "from ase.md import MDLogger\n",
    "from ase.md.nvtberendsen import NVTBerendsen\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Tinit = 9000  # K\n",
    "Tfin = 300  # K\n",
    "\n",
    "md_params = {\n",
    "    \"timestep\": 0.5 * ase.units.fs,  # MD timestep\n",
    "    \"taut\": 100 * 0.5 * ase.units.fs,  # thermostat time constant\n",
    "}\n",
    "\n",
    "# starting config\n",
    "initial_frame = load_atoms(\"/path/to/file\")[-1]\n",
    "\n",
    "\n",
    "total_melt_steps = ...  # number of the mlet steps\n",
    "total_quench_steps = ...  # number of steps to go from Tinit to Tfin\n",
    "quench_temperatures = np.linspace(Tinit, Tfin, total_quench_steps // 10)\n",
    "\n",
    "# initialize velocities\n",
    "MaxwellBoltzmannDistribution(initial_frame, temperature_K=Tinit)\n",
    "Stationary(initial_frame)\n",
    "ZeroRotation(initial_frame)\n",
    "\n",
    "# initialize dynamics object\n",
    "dyn = NVTBerendsen(initial_frame, temperature_K=Tinit, **md_params)\n",
    "\n",
    "\n",
    "# write trajectory function\n",
    "def write_frame():\n",
    "    dyn.atoms.write(\n",
    "        f\"/path/to/produced/trajectory.xyz\", append=True\n",
    "    )  # make sure the extension is xyz\n",
    "\n",
    "\n",
    "dyn.attach(write_frame, interval=100)  # set the frequency of writing to trajctory file\n",
    "\n",
    "dyn.attach(\n",
    "    MDLogger(\n",
    "        dyn,  # dynamics object\n",
    "        initial_frame,  # intial configuration\n",
    "        f\"path/to/log\",\n",
    "        peratom=True,\n",
    "        mode=\"a\",\n",
    "    ),\n",
    "    interval=100,  # frequency of writing the log\n",
    ")\n",
    "\n",
    "# run the melt\n",
    "dyn.run(total_melt_steps)\n",
    "\n",
    "for t in quench_temperatures:\n",
    "    dyn.set_temperature(t)\n",
    "    dyn.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform your analysis on the produced trajectory\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphPES",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
